{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing linear probing pipeline from Ghani\n",
    "Trying to 100% simulate ghani setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7b472043d8407e92cd559f7a67eb79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1017 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb717e2c52f473bb4a7264cfa7ecbcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/339 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf55edccf964fcfbf4d5991a732db48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/339 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917af9eb6a884b6694d1cc6d4ccac7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/203 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'labels'],\n",
       "    num_rows: 1017\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from birdset.datamodule.beans_datamodule import BEANSDataModule\n",
    "from birdset.datamodule.base_datamodule import DatasetConfig\n",
    "\n",
    "datasetconfig = DatasetConfig(dataset_name='beans_watkins', hf_path='DBD-research-group/beans_watkins', hf_name='default')\n",
    "\n",
    "datamodule = BEANSDataModule(dataset=datasetconfig)\n",
    "dataset = datamodule._load_data()\n",
    "dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load model and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 12:02:33.914340: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-17 12:02:33.914446: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-17 12:02:33.914479: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-17 12:02:33.923028: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-17 12:02:36.032213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22287 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:25:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "from birdset.modules.models.perch import PerchModel\n",
    "import torch.nn as nn\n",
    "\n",
    "num_classes = 31\n",
    "sampling_rate = 32_000 \n",
    "window_length = 5\n",
    "\n",
    "perch_network = PerchModel(num_classes=num_classes, tfhub_version=4, gpu_to_use=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Batch and Preprocess the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reprocess the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "# Resample function (#! Move resampler out)\n",
    "# Get embeddings\n",
    "def get_embedding(audio):\n",
    "    # Get waveform and sampling rate\n",
    "    waveform = torch.tensor(audio['array'], dtype=torch.float32)\n",
    "    dataset_sampling_rate = audio['sampling_rate']\n",
    "    # Resample audio\n",
    "    audio = resample_audio(waveform, dataset_sampling_rate, sampling_rate)\n",
    "    \n",
    "    # Zero-padding\n",
    "    audio = zero_pad(waveform)\n",
    "    \n",
    "    # Check if audio is too long \n",
    "    if waveform.shape[0] > window_length * sampling_rate:\n",
    "        return frame_and_average(waveform)    \n",
    "    else:\n",
    "        return perch_network.get_embeddings(audio)[0] # To just use embeddings not logits\n",
    "\n",
    "# Resample function\n",
    "def resample_audio(audio, orig_sr, target_sr):\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=orig_sr, new_freq=target_sr)\n",
    "    return resampler(audio)\n",
    "\n",
    "# Zero-padding function\n",
    "def zero_pad(audio):\n",
    "    desired_num_samples = window_length * sampling_rate \n",
    "    current_num_samples = audio.shape[0]\n",
    "    padding = desired_num_samples - current_num_samples\n",
    "    if padding > 0:\n",
    "        #print('padding')\n",
    "        pad_left = padding // 2\n",
    "        pad_right = padding - pad_left\n",
    "        audio = torch.nn.functional.pad(audio, (pad_left, pad_right))\n",
    "    return audio\n",
    "\n",
    "# Average multiple embeddings function\n",
    "def frame_and_average(audio):\n",
    "    # Ensure the waveform is mono\n",
    "    #if audio.size(0) > 1:\n",
    "        #print(\"What\")\n",
    "        #audio = audio.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    # Frame the audio\n",
    "    frame_size = window_length * sampling_rate\n",
    "    hop_size = window_length * sampling_rate\n",
    "    frames = audio.unfold(0, frame_size, hop_size)\n",
    "    \n",
    "    # Generate embeddings for each frame\n",
    "    l = []\n",
    "    for frame in frames:\n",
    "        embedding = perch_network.get_embeddings(frame) \n",
    "        l.append(embedding[0]) # To just use embeddings not logits\n",
    "    \n",
    "    embeddings = torch.stack(tuple(l))\n",
    "    \n",
    "    # Average the embeddings\n",
    "    averaged_embedding = embeddings.mean(dim=0)\n",
    "    \n",
    "    return averaged_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['audio', 'labels'])\n",
      "tensor([[[ 1.0092e-02,  2.1218e-01,  3.0808e-03,  ...,  5.2571e-05,\n",
      "           1.1198e-01, -2.5705e-02]],\n",
      "\n",
      "        [[-6.4186e-02, -2.8728e-02,  1.0271e-01,  ...,  1.0650e-01,\n",
      "           6.4827e-02, -7.4666e-03]],\n",
      "\n",
      "        [[ 2.4990e-02, -8.8441e-02, -2.1365e-02,  ..., -3.5433e-03,\n",
      "           2.9607e-02, -6.0116e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.4528e-02, -1.7473e-02,  1.9105e-02,  ...,  4.8836e-02,\n",
      "           2.3408e-01,  3.1219e-02]],\n",
      "\n",
      "        [[-2.4960e-02,  1.1930e-01,  2.1271e-02,  ...,  3.5058e-03,\n",
      "           5.9749e-03, -3.6126e-02]],\n",
      "\n",
      "        [[-6.3046e-02,  1.5308e-01,  3.1311e-02,  ..., -2.0878e-03,\n",
      "           6.5515e-02,  2.8494e-02]]])\n",
      "tensor([19, 17, 22, 23,  8, 12, 27, 29, 12,  8, 18, 12, 20,  8, 21,  9,  0,  7,\n",
      "         9, 21, 10, 12, 21,  8, 12, 26, 25, 27, 14,  2, 12, 15])\n",
      "torch.Size([32, 1, 1280])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def preprocess(item):\n",
    "    audio = item['audio']\n",
    "    return get_embedding(audio)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_new = {}\n",
    "    audios = [preprocess(item) for item in batch]\n",
    "    batch_new['audio'] =  torch.stack(tuple(audios), dim=0)\n",
    "    batch_new['labels'] = torch.tensor([item['labels'] for item in batch])\n",
    "    return batch_new\n",
    "\n",
    "train_loader = DataLoader(dataset['train'], batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(dataset['test'], batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(dataset['valid'], batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Example of iterating through the DataLoader\n",
    "for batch in train_loader:\n",
    "    print(batch.keys())\n",
    "    print(batch['audio'])\n",
    "    print(batch['labels'])\n",
    "    print(batch['audio'].shape)    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  22%|██▏       | 7/32 [00:04<00:13,  1.86it/s]"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define your classifier model\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.squeeze(1)\n",
    "        return x\n",
    "\n",
    "# Set the input size and number of classes\n",
    "input_size = 1280  # Update with your desired input size\n",
    "num_classes = 31  # Update with your desired number of classes\n",
    "\n",
    "# Create an instance of your classifier model\n",
    "classifier = Classifier(input_size, num_classes).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(classifier.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "# Set the number of training epochs\n",
    "num_epochs = 50  \n",
    "\n",
    "early_stopping_patience = 5\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    classifier.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        inputs = batch['audio'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for this epoch\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validate the model (assuming you have a validation loader)\n",
    "    classifier.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch['audio'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = classifier(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save the best model weights if necessary\n",
    "        torch.save(classifier.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # Get the inputs and labels from the batch\n",
    "        inputs = batch['audio']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = classifier(inputs)\n",
    "        outputs = outputs.squeeze(1)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "    # Print the loss for this epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "AUROC: 0.7451189756393433\n",
      "F1: 0.11209439486265182\n",
      "T1Accuracy: 0.11209439486265182\n",
      "T3Accuracy: 0.2979350984096527\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import torchmetrics\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# Set the model to evaluation mode\n",
    "perch_network.eval()\n",
    "\n",
    "\n",
    "# Initialize the metrics\n",
    "metrics = torchmetrics.MetricCollection({\n",
    "    'T1Accuracy': torchmetrics.Accuracy(\n",
    "        task=\"multiclass\",\n",
    "        num_classes=num_classes,\n",
    "        top_k=1\n",
    "    ),\n",
    "    'T3Accuracy': torchmetrics.Accuracy(\n",
    "        task=\"multiclass\",\n",
    "        num_classes=num_classes,\n",
    "        top_k=3\n",
    "    ),\n",
    "    'AUROC': torchmetrics.AUROC(\n",
    "        task=\"multiclass\",\n",
    "        num_classes=num_classes,\n",
    "        average='macro'\n",
    "    ),\n",
    "    'F1': torchmetrics.F1Score(\n",
    "        task=\"multiclass\",\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "})\n",
    "\n",
    "# Iterate over the test_loader\n",
    "for batch in test_loader:\n",
    "    # Move the batch to the device\n",
    "    #batch = {key: value.to(device) for key, value in batch.items()}\n",
    "    \n",
    "    # Forward pass\n",
    "    inputs = batch['audio']\n",
    "    labels = batch['labels']\n",
    "    with torch.no_grad():\n",
    "        outputs = classifier(inputs)\n",
    "        outputs = outputs.squeeze(1)\n",
    "    \n",
    "    # Update the metrics\n",
    "    metrics(outputs, labels)\n",
    "\n",
    "# Compute and print the metric values\n",
    "metric_values = metrics.compute()\n",
    "for metric_name, metric_value in metric_values.items():\n",
    "    print(f\"{metric_name}: {metric_value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
