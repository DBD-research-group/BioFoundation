{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing linear probing pipeline from Ghani\n",
    "Trying to 100% simulate ghani setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'labels'],\n",
       "    num_rows: 1017\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from birdset.datamodule.beans_datamodule import BEANSDataModule\n",
    "from birdset.datamodule.base_datamodule import DatasetConfig\n",
    "\n",
    "datasetconfig = DatasetConfig(dataset_name='beans_watkins', hf_path='DBD-research-group/beans_watkins', hf_name='default')\n",
    "\n",
    "datamodule = BEANSDataModule(dataset=datasetconfig)\n",
    "dataset = datamodule._load_data()\n",
    "dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be used to check the class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': 'Mac-3-A-3.wav', 'array': array([ 2.13882100e-04,  4.85118391e-04, -2.17375666e-04, ...,\n",
      "        9.30107664e-04,  7.66232726e-04,  6.42658269e-05]), 'sampling_rate': 32000}, 'labels': 9}\n",
      "{9: 51, 5: 13, 2: 43, 8: 20, 7: 44, 0: 91, 1: 31, 4: 25, 6: 27, 3: 70}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(dataset['train'][0])\n",
    "label_counts = dict(Counter(dataset['train']['labels']))\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Only use next two cells if intended) This part is for removing specific classes from watkins (for this the conversion from class name to int in beans_datamodule has to be commented out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Fin,_Finback_Whale'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m labels_to_remove \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFin,_Finback_Whale\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNorthern_Right_Whale\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels_to_remove:\n\u001b[0;32m---> 13\u001b[0m     filtered_labels[label] \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_counts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Create a new dataset excluding the filtered labels\u001b[39;00m\n\u001b[1;32m     16\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m example: example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m filtered_labels)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Fin,_Finback_Whale'"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#! Here we remove all labels that have less than x examples\n",
    "x = 15 \n",
    "label_counts = dict(Counter(dataset['train']['labels']))\n",
    "\n",
    "filtered_labels = {label: count for label, count in label_counts.items() if count < x}\n",
    "print(filtered_labels)\n",
    "# Remove additional labels\n",
    "labels_to_remove = ['Fin,_Finback_Whale', 'Northern_Right_Whale']\n",
    "\n",
    "for label in labels_to_remove:\n",
    "    filtered_labels[label] = label_counts[label]\n",
    "    \n",
    "# Create a new dataset excluding the filtered labels\n",
    "dataset = dataset.filter(lambda example: example['labels'] not in filtered_labels)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebec403dc9e9453bbfbf2280f068b102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1017 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5d922bef96400d872e0f6255742dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/339 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b4970e4f8b43ae9c668b71e4f3a60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/339 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03cf32cc65a401591f5e1e8b5ea698f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/203 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "# Convert labels back to ids\n",
    "labels = set()\n",
    "for split in dataset.keys():\n",
    "    labels.update(dataset[split][\"labels\"])\n",
    "\n",
    "label_to_id = {lbl: i for i, lbl in enumerate(labels)}\n",
    "\n",
    "def label_to_id_fn(batch):\n",
    "    for i in range(len(batch['labels'])):\n",
    "        batch['labels'][i] = label_to_id[batch['labels'][i]]\n",
    "    return batch\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    label_to_id_fn,\n",
    "    batched=True,\n",
    "    batch_size=500,\n",
    "    load_from_cache_file=True\n",
    ")\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load model and set parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from birdset.modules.models.perch import PerchModel\n",
    "import torch.nn as nn\n",
    "\n",
    "num_classes = 31 #! Don't forget to change this\n",
    "sampling_rate = 32_000 # Try 48_000 here\n",
    "window_length = 5\n",
    "input_size = 1280\n",
    "\n",
    "perch_network = PerchModel(num_classes=num_classes, tfhub_version=4, gpu_to_use=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BirdNET:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_BLOCK_4-2_ACT_1_layer_call_and_return_conditional_losses_49739) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-1_SE_CONV_1_layer_call_and_return_conditional_losses_21100) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-4_ACT_2_layer_call_and_return_conditional_losses_22243) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-5_ACT_2_layer_call_and_return_conditional_losses_49087) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_model_1_layer_call_and_return_conditional_losses_43539) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-5_SE_CONV_1_layer_call_and_return_conditional_losses_22447) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_1-1_ACT_1_layer_call_and_return_conditional_losses_44678) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-4_SE_CONV_1_layer_call_and_return_conditional_losses_22275) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-4_ACT_1_layer_call_and_return_conditional_losses_46900) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-1_ACT_1_layer_call_and_return_conditional_losses_49367) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-2_SE_CONV_1_layer_call_and_return_conditional_losses_49917) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-1_SE_CONV_1_layer_call_and_return_conditional_losses_21774) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-1_SE_CONV_1_layer_call_and_return_conditional_losses_47497) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-2_ACT_1_layer_call_and_return_conditional_losses_47691) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-3_ACT_1_layer_call_and_return_conditional_losses_50158) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-3_ACT_2_layer_call_and_return_conditional_losses_22071) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-1_ACT_2_layer_call_and_return_conditional_losses_21068) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-4_SE_CONV_1_layer_call_and_return_conditional_losses_23121) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-3_ACT_2_layer_call_and_return_conditional_losses_22917) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-1_SE_CONV_1_layer_call_and_return_conditional_losses_45868) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-3_SE_CONV_1_layer_call_and_return_conditional_losses_50336) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-2_SE_CONV_1_layer_call_and_return_conditional_losses_21931) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-2_ACT_2_layer_call_and_return_conditional_losses_22745) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-4_SE_CONV_1_layer_call_and_return_conditional_losses_21601) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-1_ACT_1_layer_call_and_return_conditional_losses_21025) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-2_SE_CONV_1_layer_call_and_return_conditional_losses_22777) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-3_ACT_1_layer_call_and_return_conditional_losses_22875) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-2_ACT_2_layer_call_and_return_conditional_losses_46201) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-3_ACT_2_layer_call_and_return_conditional_losses_21397) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-2_SE_CONV_1_layer_call_and_return_conditional_losses_47869) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-1_SE_CONV_1_layer_call_and_return_conditional_losses_49545) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-3_SE_CONV_1_layer_call_and_return_conditional_losses_21429) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_1-1_ACT_2_layer_call_and_return_conditional_losses_44817) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-3_SE_CONV_1_layer_call_and_return_conditional_losses_22103) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-1_ACT_2_layer_call_and_return_conditional_losses_47458) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_model_1_layer_call_and_return_conditional_losses_41786) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-3_SE_CONV_1_layer_call_and_return_conditional_losses_22949) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_1-2_ACT_1_layer_call_and_return_conditional_losses_44984) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-4_ACT_1_layer_call_and_return_conditional_losses_22201) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-3_ACT_1_layer_call_and_return_conditional_losses_46481) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-3_ACT_1_layer_call_and_return_conditional_losses_48110) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-2_ACT_2_layer_call_and_return_conditional_losses_49878) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-3_ACT_2_layer_call_and_return_conditional_losses_46620) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_1-2_ACT_2_layer_call_and_return_conditional_losses_45123) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_1-3_ACT_1_layer_call_and_return_conditional_losses_20907) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-1_ACT_2_layer_call_and_return_conditional_losses_22588) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-2_SE_CONV_1_layer_call_and_return_conditional_losses_46240) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-1_ACT_1_layer_call_and_return_conditional_losses_22545) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-4_ACT_1_layer_call_and_return_conditional_losses_21527) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_embeddings_13070) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-3_ACT_1_layer_call_and_return_conditional_losses_22029) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-5_ACT_1_layer_call_and_return_conditional_losses_48948) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-1_ACT_1_layer_call_and_return_conditional_losses_47319) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_1-1_ACT_1_layer_call_and_return_conditional_losses_20685) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-3_ACT_1_layer_call_and_return_conditional_losses_21355) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-2_ACT_2_layer_call_and_return_conditional_losses_21899) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-4_ACT_2_layer_call_and_return_conditional_losses_47039) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-5_SE_CONV_1_layer_call_and_return_conditional_losses_49126) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-1_ACT_2_layer_call_and_return_conditional_losses_45829) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-3_SE_CONV_1_layer_call_and_return_conditional_losses_48288) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_37532) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-4_ACT_2_layer_call_and_return_conditional_losses_48668) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-1_SE_CONV_1_layer_call_and_return_conditional_losses_22620) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_1-2_ACT_2_layer_call_and_return_conditional_losses_20831) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-4_ACT_1_layer_call_and_return_conditional_losses_50577) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-4_SE_CONV_1_layer_call_and_return_conditional_losses_47078) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-2_ACT_1_layer_call_and_return_conditional_losses_46062) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-2_ACT_1_layer_call_and_return_conditional_losses_21183) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-3_ACT_2_layer_call_and_return_conditional_losses_50297) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_basic_11033) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-4_ACT_2_layer_call_and_return_conditional_losses_21569) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-5_ACT_1_layer_call_and_return_conditional_losses_22373) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-2_ACT_2_layer_call_and_return_conditional_losses_21225) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-2_ACT_2_layer_call_and_return_conditional_losses_47830) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_39299) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-4_ACT_2_layer_call_and_return_conditional_losses_50716) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-3_ACT_2_layer_call_and_return_conditional_losses_48249) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-1_ACT_1_layer_call_and_return_conditional_losses_21699) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-4_ACT_2_layer_call_and_return_conditional_losses_23089) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-5_ACT_2_layer_call_and_return_conditional_losses_22415) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-1_ACT_2_layer_call_and_return_conditional_losses_49506) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-2_ACT_1_layer_call_and_return_conditional_losses_21857) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_1-2_ACT_1_layer_call_and_return_conditional_losses_20789) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-4_ACT_1_layer_call_and_return_conditional_losses_48529) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-2_ACT_1_layer_call_and_return_conditional_losses_22703) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-2_SE_CONV_1_layer_call_and_return_conditional_losses_21257) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference__wrapped_model_15110) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_1-3_ACT_1_layer_call_and_return_conditional_losses_45337) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-4_SE_CONV_1_layer_call_and_return_conditional_losses_48707) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_1-3_ACT_2_layer_call_and_return_conditional_losses_45476) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-3_SE_CONV_1_layer_call_and_return_conditional_losses_46659) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_3-1_ACT_2_layer_call_and_return_conditional_losses_21742) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_1-3_ACT_2_layer_call_and_return_conditional_losses_20949) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-4_ACT_1_layer_call_and_return_conditional_losses_23047) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_4-4_SE_CONV_1_layer_call_and_return_conditional_losses_50755) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_1-1_ACT_2_layer_call_and_return_conditional_losses_20728) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_BLOCK_2-1_ACT_1_layer_call_and_return_conditional_losses_45690) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "from birdset.modules.models.birdnet import BirdNetModel\n",
    "import torch.nn as nn\n",
    "\n",
    "num_classes = 31 #! Don't forget to change this\n",
    "sampling_rate = 48_000 \n",
    "window_length = 3\n",
    "input_size = 1024\n",
    "\n",
    "perch_network = BirdNetModel(num_classes=num_classes, model_path='../../checkpoints/birdnet/BirdNET_GLOBAL_6K_V2.4_Model', train_classifier=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Batch and Preprocess the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. k-sample the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the training set: 1017\n",
      "Number of samples in the validation set: 339\n",
      "Number of samples in the testing set: 339\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of samples in the training set:\", len(dataset['train']))\n",
    "print(\"Number of samples in the validation set:\", len(dataset['valid']))\n",
    "print(\"Number of samples in the testing set:\", len(dataset['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the training set: 940\n",
      "Number of samples in the validation set: 378\n",
      "Number of samples in the testing set: 377\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from datasets import concatenate_datasets, DatasetDict, Dataset\n",
    "\n",
    "# Define the number of samples per class\n",
    "samples_per_class = 32\n",
    "\n",
    "# Merge the train, valid, and test splits\n",
    "merged_data = concatenate_datasets([dataset['train'], dataset['valid'], dataset['test']])\n",
    "merged_data = merged_data.shuffle()\n",
    "\n",
    "# Create a dictionary to store the selected samples per class\n",
    "selected_samples = defaultdict(list)\n",
    "rest_samples = []\n",
    "# Iterate over the merged data and select the desired number of samples per class\n",
    "for sample in merged_data:\n",
    "    label = sample['labels']\n",
    "    if len(selected_samples[label]) < samples_per_class:\n",
    "        selected_samples[label].append(sample)\n",
    "    else:\n",
    "        rest_samples.append(sample)    \n",
    "\n",
    "# Flatten the selected samples into a single list\n",
    "selected_samples = [sample for samples in selected_samples.values() for sample in samples]\n",
    "\n",
    "# Split the selected samples into training, validation, and testing sets\n",
    "test_ratio = 0.5\n",
    "\n",
    "num_samples = len(rest_samples)\n",
    "num_test_samples = int(test_ratio * num_samples)\n",
    "\n",
    "train_data = selected_samples\n",
    "test_data = rest_samples[:num_test_samples]\n",
    "val_data = rest_samples[num_test_samples:]\n",
    "\n",
    "train_data = Dataset.from_dict({key: [sample[key] for sample in train_data] for key in train_data[0]})\n",
    "test_data = Dataset.from_dict({key: [sample[key] for sample in test_data] for key in test_data[0]})\n",
    "val_data = Dataset.from_dict({key: [sample[key] for sample in val_data] for key in val_data[0]})\n",
    "\n",
    "# Print the number of samples in each split\n",
    "print(\"Number of samples in the training set:\", len(train_data))\n",
    "print(\"Number of samples in the validation set:\", len(val_data))\n",
    "print(\"Number of samples in the testing set:\", len(test_data))\n",
    "\n",
    "# Combine into a DatasetDict\n",
    "datasett = DatasetDict({\n",
    "    'train': train_data,\n",
    "    'valid': val_data,\n",
    "    'test': test_data\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "# Resample function (#! Move resampler out)\n",
    "# Get embeddings\n",
    "def get_embedding(audio):\n",
    "    # Get waveform and sampling rate\n",
    "    waveform = torch.tensor(audio['array'], dtype=torch.float32)\n",
    "    dataset_sampling_rate = audio['sampling_rate']\n",
    "    # Resample audio\n",
    "    audio = resample_audio(waveform, dataset_sampling_rate, sampling_rate)\n",
    "    #print('Audio length:', audio.shape[0]/sampling_rate)\n",
    "    # Zero-padding\n",
    "    audio = zero_pad(waveform)\n",
    "    \n",
    "    # Check if audio is too long \n",
    "    if waveform.shape[0] > window_length * sampling_rate:\n",
    "        return frame_and_average(waveform)    \n",
    "    else:\n",
    "        return perch_network.get_embeddings(audio)[0] # To just use embeddings not logits\n",
    "\n",
    "# Resample function\n",
    "def resample_audio(audio, orig_sr, target_sr):\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=orig_sr, new_freq=target_sr)\n",
    "    return resampler(audio)\n",
    "\n",
    "# Zero-padding function\n",
    "def zero_pad(audio):\n",
    "    desired_num_samples = window_length * sampling_rate \n",
    "    current_num_samples = audio.shape[0]\n",
    "    padding = desired_num_samples - current_num_samples\n",
    "    if padding > 0:\n",
    "        #print('padding')\n",
    "        pad_left = padding // 2\n",
    "        pad_right = padding - pad_left\n",
    "        audio = torch.nn.functional.pad(audio, (pad_left, pad_right))\n",
    "    return audio\n",
    "\n",
    "# Average multiple embeddings function\n",
    "def frame_and_average(audio):\n",
    "    # Ensure the waveform is mono\n",
    "    #if audio.size(0) > 1:\n",
    "        #print(\"What\")\n",
    "        #audio = audio.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    # Frame the audio\n",
    "    frame_size = window_length * sampling_rate\n",
    "    hop_size = window_length * sampling_rate\n",
    "    frames = audio.unfold(0, frame_size, hop_size)\n",
    "    \n",
    "    # Generate embeddings for each frame\n",
    "    l = []\n",
    "    for frame in frames:\n",
    "        embedding = perch_network.get_embeddings(frame) \n",
    "        l.append(embedding[0]) # To just use embeddings not logits\n",
    "    \n",
    "    embeddings = torch.stack(tuple(l))\n",
    "    \n",
    "    # Average the embeddings\n",
    "    averaged_embedding = embeddings.mean(dim=0)\n",
    "    \n",
    "    return averaged_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 01:38:46.110718: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator jax2tf_infer_fn_/assert_equal_1/Assert/AssertGuard/Assert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['audio', 'labels'])\n",
      "tensor([[[ 0.0078,  0.0187, -0.0098,  ..., -0.0284,  0.1116, -0.0138]],\n",
      "\n",
      "        [[ 0.0024, -0.0420, -0.0278,  ...,  0.1036,  0.0731, -0.0307]],\n",
      "\n",
      "        [[-0.0604, -0.0278, -0.0089,  ...,  0.1065,  0.1766, -0.0013]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0173, -0.0715,  0.0045,  ..., -0.0214,  0.0518, -0.0411]],\n",
      "\n",
      "        [[-0.0092, -0.0189,  0.0296,  ...,  0.0156,  0.2073, -0.0058]],\n",
      "\n",
      "        [[ 0.0504,  0.0031,  0.0155,  ...,  0.0448,  0.1135,  0.0121]]])\n",
      "tensor([30, 25, 10,  1, 19, 13, 15, 29,  8, 28,  2,  7,  2,  5, 16, 24,  4, 10,\n",
      "        23, 25, 22, 13, 14, 24, 18, 20, 26, 18, 22, 15, 17, 19])\n",
      "torch.Size([32, 1, 1280])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def preprocess(item):\n",
    "    audio = item['audio']\n",
    "    return get_embedding(audio)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_new = {}\n",
    "    audios = [preprocess(item) for item in batch]\n",
    "    batch_new['audio'] =  torch.stack(tuple(audios), dim=0)\n",
    "    \n",
    "    #batch_new['labels'] = torch.stack([torch.nn.functional.one_hot(torch.tensor(item['labels'],  dtype=torch.long), num_classes=num_classes) for item in batch]).float() #* For one hot-encoding \n",
    "    batch_new['labels'] = torch.tensor([item['labels'] for item in batch])\n",
    "    return batch_new\n",
    "\n",
    "train_loader = DataLoader(dataset['train'], batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(dataset['test'], batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(dataset['valid'], batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Example of iterating through the DataLoader\n",
    "for batch in train_loader:\n",
    "    print(batch.keys())\n",
    "    print(batch['audio'])\n",
    "    print(batch['labels'])\n",
    "    print(batch['audio'].shape)    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "gpu_id = 0  # Change this to the ID of the GPU you want to use\n",
    "device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "print(f'Using device: {device}') #! Not working right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|██████████| 4/4 [00:01<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Train Loss: 3.4358, Val Loss: 3.4363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|██████████| 4/4 [00:02<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25, Train Loss: 3.4341, Val Loss: 3.4358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|██████████| 4/4 [00:01<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25, Train Loss: 3.4331, Val Loss: 3.4353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|██████████| 4/4 [00:01<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25, Train Loss: 3.4326, Val Loss: 3.4348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|██████████| 4/4 [00:01<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25, Train Loss: 3.4311, Val Loss: 3.4343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|██████████| 4/4 [00:01<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25, Train Loss: 3.4306, Val Loss: 3.4338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|██████████| 4/4 [00:02<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25, Train Loss: 3.4298, Val Loss: 3.4333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|██████████| 4/4 [00:01<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25, Train Loss: 3.4290, Val Loss: 3.4329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|██████████| 4/4 [00:01<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25, Train Loss: 3.4274, Val Loss: 3.4324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|██████████| 4/4 [00:01<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25, Train Loss: 3.4268, Val Loss: 3.4319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|██████████| 4/4 [00:02<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25, Train Loss: 3.4255, Val Loss: 3.4314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|██████████| 4/4 [00:01<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25, Train Loss: 3.4244, Val Loss: 3.4309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|██████████| 4/4 [00:02<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25, Train Loss: 3.4239, Val Loss: 3.4304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|██████████| 4/4 [00:02<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25, Train Loss: 3.4227, Val Loss: 3.4299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|██████████| 4/4 [00:01<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25, Train Loss: 3.4221, Val Loss: 3.4294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|██████████| 4/4 [00:01<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25, Train Loss: 3.4206, Val Loss: 3.4289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|██████████| 4/4 [00:01<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25, Train Loss: 3.4197, Val Loss: 3.4284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|██████████| 4/4 [00:01<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25, Train Loss: 3.4185, Val Loss: 3.4279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|██████████| 4/4 [00:02<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25, Train Loss: 3.4179, Val Loss: 3.4274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|██████████| 4/4 [00:01<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25, Train Loss: 3.4164, Val Loss: 3.4269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|██████████| 4/4 [00:01<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25, Train Loss: 3.4157, Val Loss: 3.4264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|██████████| 4/4 [00:02<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25, Train Loss: 3.4152, Val Loss: 3.4259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|██████████| 4/4 [00:01<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25, Train Loss: 3.4140, Val Loss: 3.4254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|██████████| 4/4 [00:01<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25, Train Loss: 3.4129, Val Loss: 3.4250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|██████████| 4/4 [00:02<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train Loss: 3.4117, Val Loss: 3.4245\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define your classifier model\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, num_classes)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1) #* self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(1)\n",
    "        x = self.log_softmax(self.fc(x))\n",
    "        return x\n",
    "\n",
    "# Create an instance of your classifier model\n",
    "classifier = Classifier(input_size, num_classes).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss() #* nn.BCELoss()\n",
    "optimizer = optim.AdamW(classifier.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "# Set the number of training epochs\n",
    "num_epochs = 25\n",
    "\n",
    "early_stopping_patience = 5\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    classifier.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        inputs = batch['audio'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for this epoch\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validate the model (assuming you have a validation loader)\n",
    "    classifier.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch['audio'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = classifier(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save the best model weights if necessary\n",
    "        torch.save(classifier.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.6013455390930176\n",
      "F1: 0.05605095624923706\n",
      "T1Accuracy: 0.05605095624923706\n",
      "T3Accuracy: 0.1719745248556137\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import torchmetrics\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "perch_network.eval()\n",
    "\n",
    "\n",
    "# Initialize the metrics\n",
    "metrics = torchmetrics.MetricCollection({\n",
    "    'T1Accuracy': torchmetrics.Accuracy(\n",
    "        task=\"multiclass\",\n",
    "        num_classes=num_classes,\n",
    "        top_k=1\n",
    "    ),\n",
    "    'T3Accuracy': torchmetrics.Accuracy(\n",
    "        task=\"multiclass\",\n",
    "        num_classes=num_classes,\n",
    "        top_k=3\n",
    "    ),\n",
    "    'AUROC': torchmetrics.AUROC(\n",
    "        task=\"multiclass\",\n",
    "        num_classes=num_classes,\n",
    "        average='macro'\n",
    "    ),\n",
    "    'F1': torchmetrics.F1Score(\n",
    "        task=\"multiclass\",\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "}).to(device)\n",
    "\n",
    "# Iterate over the test_loader\n",
    "for batch in test_loader:\n",
    "    # Forward pass\n",
    "    inputs = batch['audio'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    #labels = torch.argmax(labels, dim=1) #* For one hot-encoding \n",
    "    with torch.no_grad():\n",
    "        outputs = classifier(inputs)\n",
    "        outputs = outputs.squeeze(1)\n",
    "    \n",
    "    # Update the metrics\n",
    "    metrics(outputs, labels)\n",
    "\n",
    "# Compute and print the metric values\n",
    "metric_values = metrics.compute()\n",
    "for metric_name, metric_value in metric_values.items():\n",
    "    print(f\"{metric_name}: {metric_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Ghani datasets</u>\n",
    "\n",
    "|Dataset|Classes|Available?|\n",
    "|-------|-------|----------|\n",
    "|Godwit Calls|5|No part of a master thesis|\n",
    "|Yellowhammer Dialects|2|Probably not (Only two classes anyway)|\n",
    "|Bats|5|Yes but pitch shifting and two sources of which one is private|\n",
    "|Watkins|32|Yes but removed some classes|\n",
    "|RFCX Frog & Bird|12+13|Yes but for detection and not split in BEANS|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Results with Perch</u>\n",
    "These are the results in this isolated run whereas we compare them to the Birdset Pipeline setup. We used 25 Epochs.\n",
    "| Dataset         | Classes|AUROC (BirdsetPipeline results) | T1 (B.P.) | Audio lengths |Samples per class|\n",
    "|--------------------|---|---------------------|-----------------|-----|----|\n",
    "| beans_watkins      |31|**89** (85)                   |**32%** (23%)|Different lengths 1-45s|~30|\n",
    "| beans_bats         |10|**79**  (78)                  |38% (**39%**)|0-5s|600|\n",
    "| beans_cbi          |264|   (96)                 |(51%)|4-10s (Mostly 10)|~50-70|\n",
    "| beans_dogs         |10|**78**    (75)                |**28%** (31%)|2-30s|13-70|\n",
    "| beans_humbugdb     |14|**69** (66)|**46%** (12%)|1-55s|~70 or ~400|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Results with BirdNET</u>\n",
    "Used 25 Epochs\n",
    "| Dataset         |AUROC | T1 |\n",
    "|------------------|---|---|\n",
    "| beans_watkins      |84|23%|\n",
    "| beans_bats         |||\n",
    "| beans_cbi          |||\n",
    "| beans_dogs         |||\n",
    "| beans_humbugdb     |||"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
