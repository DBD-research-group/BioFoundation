{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed02a1b-72ee-475e-9794-a6c0396fea10",
   "metadata": {},
   "source": [
    "# GADME Data Pipeline Tutorial\n",
    "\n",
    "This Jupyter notebook provides a comprehensive guide to setting up and configuring a data pipeline tailored for bird classification in audio files. The tutorial is structured to walk you through each component of the pipeline, ensuring a clear understanding of its functionality and configuration. Whether you are processing raw audio data or spectrograms, this notebook aims to provide you with the necessary knowledge to efficiently set up your data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909c001-7d00-4a0f-a16c-c984e7d91740",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "### Prerequisites\n",
    "Before initiating the installation process of the GADME pipeline, it's crucial to ensure that your computing environment meets the following prerequisites:\n",
    "- **Python**: You should have Python version 3.10 or higher installed on your system.\n",
    "\n",
    "### Installation Steps\n",
    "The GADME pipeline can be installed using either of the two methods: via Conda with Pip, or using Poetry. Select the method that best suits your preference and follow the corresponding steps below.\n",
    "\n",
    "#### Using Conda and Pip\n",
    "\n",
    "1. **Create a Conda Environment**: Begin by setting up a dedicated environment for GADME. This is a best practice to manage dependencies and avoid potential conflicts with other packages in your system.\n",
    "\n",
    "   ```bash\n",
    "   conda create -n gadme python=3.10\n",
    "   ```\n",
    "\n",
    "   After the environment is successfully created, activate it:\n",
    "\n",
    "   ```bash\n",
    "   conda activate gadme\n",
    "   ```\n",
    "\n",
    "2. **Install GADME**: Proceed with cloning the GADME repository and installing the package in editable mode. This approach is beneficial as it allows any modifications you make to the GADME code to be reflected immediately without the need for reinstallation.\n",
    "\n",
    "   ```bash\n",
    "   git clone https://github.com/DBD-research-group/GADME.git\n",
    "   cd GADME\n",
    "   pip install -e .\n",
    "   ```\n",
    "\n",
    "#### Using Poetry\n",
    "\n",
    "1. **Clone the Repository**: Start with cloning the GADME repository to your local machine and navigate to the cloned directory.\n",
    "\n",
    "   ```bash\n",
    "   git clone https://github.com/DBD-research-group/GADME.git\n",
    "   cd GADME\n",
    "   ```\n",
    "\n",
    "2. **Configure Poetry**: Prepare the project for Poetry by renaming the `pyproject.poetry` file to `pyproject.toml`.\n",
    "\n",
    "   ```bash\n",
    "   mv pyproject.poetry pyproject.toml\n",
    "   ```\n",
    "\n",
    "3. **Install Dependencies and Activate Environment**: Install all the necessary dependencies with Poetry and then activate the Poetry shell environment.\n",
    "\n",
    "   ```bash\n",
    "   poetry install\n",
    "   poetry shell\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a4f398-f18d-42e4-a6b5-02329f1f5567",
   "metadata": {},
   "source": [
    "## Log in to Huggingface\n",
    "\n",
    "Our datasets are shared via HuggingFace Datasets in our [HuggingFace GADME repository](https://huggingface.co/datasets/DBD-research-group/gadme_v1). Huggingface is a central hub for sharing and utilizing datasets and models, particularly beneficial for machine learning and data science projects. For accessing private datasets hosted on HuggingFace, you need to be authenticated. Here's how you can log in to HuggingFace:\n",
    "\n",
    "1. **Install HuggingFace CLI**: If you haven't already, you need to install the HuggingFace CLI (Command Line Interface). This tool enables you to interact with HuggingFace services directly from your terminal. You can install it using pip:\n",
    "\n",
    "   ```bash\n",
    "   pip install huggingface_hub\n",
    "   ```\n",
    "\n",
    "2. **Login via CLI**: Once the HuggingFace CLI is installed, you can log in to your HuggingFace account directly from your terminal. This step is essential for accessing private datasets or contributing to the HuggingFace community. Use the following command:\n",
    "\n",
    "   ```bash\n",
    "   huggingface-cli login\n",
    "   ```\n",
    "\n",
    "   After executing this command, you'll be prompted to enter your HuggingFace credentials ([User Access Token](https://huggingface.co/docs/hub/security-tokens)). Once authenticated, your credentials will be saved locally, allowing seamless access to HuggingFace resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c29ea2",
   "metadata": {},
   "source": [
    "## TLDR;\n",
    "To get started with the default configuration, you can use the following code snippet to set up the GADME pipeline to load the [High Sierras](https://zenodo.org/records/7525805) test dataset (10,296 samples) including a train set (5,197 samples) with matching bird classes from [xeno-canto](https://xeno-canto.org/). The total size of the dataset is 6.2GB. The samples will be provided as spectrograms with a resolution of `128x1024` pixels in batches of size `32`, the labels are one-hot encoded for a multilabel classification task. Down below you find further information on how to configure the pipeline to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f345d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940e7a92b7e84daaa95948191b49629d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/11.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find a dataset script at /workspace/dataset/notebooks/tutorials/DBD-research-group/gadme/gadme.py or any data file in the same directory. Couldn't find 'DBD-research-group/gadme' on the Hugging Face Hub either: FileNotFoundError: No (supported) data files or dataset script found in DBD-research-group/gadme. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m dm \u001b[38;5;241m=\u001b[39m GADMEDataModule()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# prepare the data (download dataset, ...)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# setup the dataloaders\u001b[39;00m\n\u001b[1;32m      8\u001b[0m dm\u001b[38;5;241m.\u001b[39msetup(stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspace/src/datamodule/base_datamodule.py:194\u001b[0m, in \u001b[0;36mBaseDataModuleHF.prepare_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    192\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrepare Data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 194\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_data(dataset)\n\u001b[1;32m    196\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_splits(dataset)\n",
      "File \u001b[0;32m/workspace/src/datamodule/gadme_datamodule.py:78\u001b[0m, in \u001b[0;36mGADMEDataModule._load_data\u001b[0;34m(self, decode)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, decode: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     67\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    Loads the data.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m        The loaded data.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/src/datamodule/base_datamodule.py:299\u001b[0m, in \u001b[0;36mBaseDataModuleHF._load_data\u001b[0;34m(self, decode)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03mLoad audio dataset from Hugging Face Datasets.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03mReturns HF dataset with audio column casted to Audio feature, containing audio data as numpy array and sampling rate.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    297\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m> Loading data set.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 299\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, IterableDataset \u001b[38;5;241m|\u001b[39mIterableDatasetDict):\n\u001b[1;32m    306\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterable datasets not supported yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/src-xS3fZVNL-py3.10/lib/python3.10/site-packages/datasets/load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2125\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2126\u001b[0m )\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2129\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/src-xS3fZVNL-py3.10/lib/python3.10/site-packages/datasets/load.py:1815\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1813\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1814\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1815\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1822\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1823\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1824\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/src-xS3fZVNL-py3.10/lib/python3.10/site-packages/datasets/load.py:1508\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m-> 1508\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1509\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1510\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1511\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a dataset script at /workspace/dataset/notebooks/tutorials/DBD-research-group/gadme/gadme.py or any data file in the same directory. Couldn't find 'DBD-research-group/gadme' on the Hugging Face Hub either: FileNotFoundError: No (supported) data files or dataset script found in DBD-research-group/gadme. "
     ]
    }
   ],
   "source": [
    "from src.datamodule.gadme_datamodule import GADMEDataModule\n",
    "\n",
    "# initiate the data module\n",
    "dm = GADMEDataModule()\n",
    "# prepare the data (download dataset, ...)\n",
    "dm.prepare_data()\n",
    "# setup the dataloaders\n",
    "dm.setup(stage=\"fit\")\n",
    "# get the dataloaders\n",
    "train_loader = dm.train_dataloader()\n",
    "# get the first batch\n",
    "batch = next(iter(train_loader))\n",
    "# get shape of the batch\n",
    "print(batch[\"input_values\"].shape)\n",
    "print(batch[\"labels\"].shape)\n",
    "batch\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be30bd-772f-407f-9925-ef247dbc8219",
   "metadata": {},
   "source": [
    "## Configuration of GADME Data Pipeline\n",
    "\n",
    "The GADME Data Pipeline offers a robust and flexible configuration system, primarily designed to streamline the process of setting up your data processing environment. While this notebook presents hardcoded configurations for simplicity, it's important to note that these settings can be dynamically managed using advanced configuration tools like Hydra. Hydra is a powerful utility that enables flexible and scalable configuration management, allowing you to adapt the pipeline settings to various environments or use cases seamlessly. For an in-depth understanding of Hydra, consider visiting [Hydra's official documentation](https://hydra.cc/docs/intro).\n",
    "\n",
    "Tipp! Detailed information is provided in the docstrings of the classes and functions. You can access them by hovering over the class or function name in your IDE or by opening the source file in a text editor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad76228",
   "metadata": {},
   "source": [
    "### GADMEDataModule\n",
    "The `GADMEDataModule` is a [PyTorch Lightning DataModule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html#lightningdatamodule) that encapsulates the entire data pipeline. It inherits from `BaseDataModuleHF` which is a base class for all DataModules that use [HuggingFace datasets libary](https://huggingface.co/docs/datasets/index).\n",
    "\n",
    "To initialize the `GADMEDataModule`, you need to provide the following parameters:\n",
    "\n",
    "```python\n",
    "from src.datamodule.gadme_datamodule import GADMEDataModule\n",
    "\n",
    "data_module = GADMEDataModule(\n",
    "    dataset=dataset_config, #dataset (DatasetConfig): The configuration for the dataset.\n",
    "    loaders=loaders_config, #loaders (LoadersConfig): The configuration for the loaders.\n",
    "    transforms=transforms, #transforms (GADMETransformsWrapper): The transforms to be applied to the data.\n",
    "    mapper=mapper #mapper (XCEventMapping): The mapping for the events.\n",
    ")\n",
    "```\n",
    "\n",
    "We will now walk through each of these parameters to understand their functionality and configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5158f248-890f-47d0-8b16-68758a314685",
   "metadata": {},
   "source": [
    "### 1. Dataset Configuration\n",
    "\n",
    "Configuring the dataset is the first step in configuring the GADME data pipeline, here you specify which dataset you want to load, how many classes it has, and how the data is split into train, validation, and test sets. The `DatasetConfig` class is used to configure the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33cca9fe-2ac0-4418-93d1-e8ad5fd37786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datamodule.base_datamodule import DatasetConfig\n",
    "\n",
    "dataset_config = DatasetConfig(\n",
    "    data_dir='../../data_birdset',\n",
    "    dataset_name='HSN',\n",
    "    hf_path='DBD-research-group/BirdSet',\n",
    "    hf_name='HSN',\n",
    "    n_classes=21,\n",
    "    n_workers=1,\n",
    "    val_split=0.2,\n",
    "    task=\"multilabel\",\n",
    "    subset=None,\n",
    "    sampling_rate=32000,\n",
    "    class_weights_sampler=None,\n",
    "    classlimit=500,\n",
    "    eventlimit=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea4694-e2e8-4b71-a2ad-6b004850f7e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here's a brief overview of the parameters used in the `DatasetConfig` class:\n",
    "\n",
    "- `data_dir`: Specifies the directory where the dataset files are stored. **Important**: The dataset uses a lot of disk space, so make sure you have enough storage available.\n",
    "- `dataset_name`: The name assigned to the dataset.\n",
    "- `hf_path`: The path to the dataset stored on HuggingFace.\n",
    "- `hf_name`: The name of the dataset on HuggingFace.\n",
    "- `seed`: A seed value for ensuring reproducibility across runs.\n",
    "- `n_classes`: The total number of distinct classes in the dataset.\n",
    "- `n_workers`: The number of worker processes used for data loading.\n",
    "- `val_split`: The proportion of the dataset reserved for validation.\n",
    "- `task`: Defines the type of task (e.g., 'multilabel' or 'multiclass').\n",
    "- `sampling_rate`: The sampling rate for audio data processing.\n",
    "- `class_weights_sampler`: Indicates whether to use class weights in the sampler for handling imbalanced datasets.\n",
    "- `class_limit`: The maximum number of samples per class.\n",
    "- `eventlimit`: Defines the maximum number of audio events processed per audio file, capping the quantity to ensure balance across files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc2a0b4-c98c-4e95-9b51-3e7f17d038eb",
   "metadata": {},
   "source": [
    "#### Important Note:\n",
    "- The `class_weights_loss` parameter is currently deprecated and only implemented for focal loss. It's recommended to utilize the `class_weights_sampler` instead, as it has shown to yield favorable results, particularly as evidenced by the winner of the [BirdCLEF 2023](https://www.kaggle.com/competitions/birdclef-2023) challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f58a9d4-aabd-43b7-b6f6-e7acbb4cefeb",
   "metadata": {},
   "source": [
    "Selecting appropriate values for these parameters is crucial, as they directly influence the efficiency of the training process and the overall performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5734f65a-8fb3-4468-8fa2-7bccff9f8097",
   "metadata": {},
   "source": [
    "### 2. Dataloader Configuration\n",
    "\n",
    "Once the dataset is configured, the next crucial step is setting up the data loaders. Data loaders are pivotal in efficiently feeding data into the model during both the training and testing phases. They manage the data flow, ensuring that the model is supplied with a consistent stream of data batches. In this section, we'll use the `LoaderConfig` and `LoadersConfig` classes to define different configurations for the training and testing data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a49150e-4ce1-4ed4-abc7-9d376eb9c3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datamodule.base_datamodule import LoaderConfig, LoadersConfig\n",
    "# Configuration for the training data loader\n",
    "train_loader_config = LoaderConfig(\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    persistent_workers=False,\n",
    "    #prefetch_factor=None,\n",
    ")\n",
    "\n",
    "# Configuration for the testing data loader\n",
    "test_loader_config = LoaderConfig(\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    persistent_workers=False,\n",
    "    #prefetch_factor=None,\n",
    ")\n",
    "\n",
    "# Aggregating the loader configurations\n",
    "loaders_config = LoadersConfig(\n",
    "    train=train_loader_config,\n",
    "    valid=test_loader_config,\n",
    "    test=test_loader_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea42652-6857-4d0e-be29-8f50928aad0b",
   "metadata": {},
   "source": [
    "Here's a brief overview of the parameters used in the `LoaderConfig` class:\n",
    "\n",
    "- `batch_size`: Specifies the number of samples contained in each batch. This is a crucial parameter as it impacts memory utilization and model performance.\n",
    "- `shuffle`: Determines whether the data is shuffled at the beginning of each epoch. Shuffling is typically used for training data to ensure model robustness and prevent overfitting.\n",
    "- `num_workers`: Sets the number of subprocesses to be used for data loading. More workers can speed up the data loading process but also increase memory consumption.\n",
    "- `pin_memory`: When set to `True`, enables the DataLoader to copy Tensors into CUDA pinned memory before returning them. This can lead to faster data transfer to CUDA-enabled GPUs.\n",
    "- `drop_last`: Determines whether to drop the last incomplete batch. Setting this to `True` is useful when the total size of the dataset is not divisible by the batch size.\n",
    "- `persistent_workers`: Indicates whether the data loader should keep the workers alive for the next epoch. This can improve performance at the cost of memory.\n",
    "- `prefetch_factor`: Defines the number of samples loaded in advance by each worker. This parameter is commented out here and can be adjusted based on specific requirements.\n",
    "\n",
    "Proper configuration of the data loaders is essential as it directly influences the efficiency of the training process, hardware resource utilization, and ultimately, the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a2390d-937a-46c8-8829-94ff3eca2b28",
   "metadata": {},
   "source": [
    "### 3. Configuration of TransformationsWrapper\n",
    "\n",
    "Transformations play a critical role in the data preparation process within the GADME data pipeline. These operations, applied before the data is fed into the model, encompass a range of augmentation techniques designed to regularize the model and prevent overfitting. Properly configured transformations not only enhance the diversity and quality of the training data but also help the model generalize better to new, unseen data.\n",
    "\n",
    "In the GADME framework, transformations are meticulously orchestrated through the `GADMETransformsWrapper` class. This wrapper acts as a comprehensive interface for defining and applying various transformations and augmentations to the data. It ensures that the data is consistently and effectively transformed, aligning with the specific requirements of the model and the inherent characteristics of the dataset.\n",
    "\n",
    "By configuring the `transforms_wrapper` using the `GADMETransformsWrapper` class, you gain precise control over how the data is manipulated during the preprocessing phase.\n",
    "\n",
    "To initialize the `GADMETransformsWrapper`, you need to provide the following parameters:\n",
    "\n",
    "```python\n",
    "from src.datamodule.components.transforms import GADMETransformsWrapper\n",
    "\n",
    "transforms = GADMETransformsWrapper(\n",
    "    task: Literal['multiclass', 'multilabel'] = \"multilabel\",\n",
    "    sampling_rate: int = 32000,\n",
    "    model_type: Literal['vision', 'waveform'] = \"waveform\",\n",
    "    spectrogram_augmentations: DictConfig = DictConfig({}),\n",
    "    waveform_augmentations: DictConfig = DictConfig({}),\n",
    "    decoding: EventDecoding | None = None,\n",
    "    feature_extractor: DefaultFeatureExtractor = DefaultFeatureExtractor(),\n",
    "    max_length: int = 5,\n",
    "    nocall_sampler: DictConfig = DictConfig({}),\n",
    "    preprocessing: PreprocessingConfig = PreprocessingConfig()\n",
    ")\n",
    "```\n",
    "\n",
    "We will go through each of these parameters to understand their functionality and configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239b8432",
   "metadata": {},
   "source": [
    "#### 3.1 Dataset and Model specific parameters\n",
    "The following parameters ensure that the transformations are tailored to the specific requirements of the dataset and the model:\n",
    "\n",
    "- `task`: Defines the type of task (e.g., 'multilabel' or 'multiclass').\n",
    "- `sampling_rate`: The sampling rate for audio data processing.\n",
    "- `model_type`: Specifies the type of model (e.g., 'vision' or 'waveform'). In case of a vison model, the input data is expected to be a spectrogram, while for a waveform model, the input data is the raw audio waveform.\n",
    "- max_length: The maximum length of the audio files in seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16af9b0-b4da-47a3-a551-433009c4739d",
   "metadata": {},
   "source": [
    "#### 3.2 Augmentations\n",
    "\n",
    "Augmentations are powerful techniques applied to the data to introduce diversity and variability. They are particularly useful in audio and signal processing to enhance the robustness of models against variations in input data. In the GADME framework, you can configure waveform and spectrogram augmentations as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306dfe3d-b6a1-41e9-ab21-ac551e501dd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Waveform Augmentations**\n",
    "\n",
    "These augmentations are applied directly to the audio waveform. In GADME, you can use any waveform augmentation technique as long as it can be composed by the [torch-audiomentations Compose function](https://github.com/asteroid-team/torch-audiomentations/blob/main/torch_audiomentations/core/composition.py). You can add waveform augmentations as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "263b67d3-0a2f-4ccf-b8eb-d55af0236ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_audiomentations import AddColoredNoise, PitchShift\n",
    "waveform_augmentation = {\n",
    "    \"colored_noise\": AddColoredNoise(p=0.2, min_snr_in_db=3.0, max_snr_in_db=30.0, min_f_decay=-2.0, max_f_decay=2.0),\n",
    "    \"pitch_shift\": PitchShift(p=0.2, sample_rate=32000, min_transpose_semitones=-4.0, max_transpose_semitones=4.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c94eee-4cd7-49eb-b277-1902b48ced9e",
   "metadata": {},
   "source": [
    "In this example:\n",
    "- `colored_noise`: Adds colored noise to the audio signal to simulate various real-world noise conditions.\n",
    "- `pitch_shift`: Alters the pitch of the audio signal, which is useful for simulating different tonal variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf77953-a58d-4e81-8e50-2d15be2ca0bf",
   "metadata": {},
   "source": [
    "**Spectrogram Augmentations**\n",
    "\n",
    "These augmentations are applied to the spectrogram representation of the audio. In GADME, you can use any spectrogram augmentation technique as long as it can be composed by the [torchvision Compose function](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html). You can add spectrogram augmentations as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58e9af4b-879b-4095-92da-99a5e0992c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomApply\n",
    "from torchaudio.transforms import TimeMasking, FrequencyMasking\n",
    "spectrogram_augmentations = {\n",
    "    \"time_masking\": RandomApply([TimeMasking(time_mask_param=100, iid_masks=True)], p=0.3),\n",
    "    \"frequency_masking\": RandomApply([FrequencyMasking(freq_mask_param=100, iid_masks=True)], p=0.5)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aca7d4-609d-4519-8675-ee7a35fa52c2",
   "metadata": {},
   "source": [
    "In this example:\n",
    "- `time_masking`: Randomly masks a sequence of consecutive time steps in the spectrogram, helping the model become invariant to small temporal shifts.\n",
    "- `frequency_masking`: Randomly masks a sequence of consecutive frequency channels, encouraging the model to be robust against frequency variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04268897-f6f4-4488-9d85-12cf23c15baf",
   "metadata": {},
   "source": [
    "Configuring the augmentations correctly is crucial as they directly influence the model's ability to learn from a diverse set of data representations, ultimately leading to better generalization and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ee00cb-31db-4f66-883f-623791389e06",
   "metadata": {},
   "source": [
    "#### 3.3 Decoding\n",
    "\n",
    "Decoding is a process, that converts the (compressed) data into a format that can be directly used by the model. In the GADME framework, we use the `EventDecoding` class by default. It is designed for preprocessing audio files in the context of event detection tasks. Its primary function is to ensure that each audio segment fed into the model is not only in the correct format, but also conditioned to improve the model's ability to identify and understand different audio events. Decoding is configured as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7e2b022-ae03-4940-99de-3bf4823ce506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datamodule.components import EventDecoding\n",
    "decoding = EventDecoding(\n",
    "    min_len=1.0,\n",
    "    max_len=5.0,\n",
    "    sampling_rate=32000,\n",
    "    extension_time=8,\n",
    "    extracted_interval=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad60adc-3a7e-4506-9c7e-529074359b9f",
   "metadata": {},
   "source": [
    "Key Parameters:\n",
    "- `_target_`: Specifies the EventDecoding component to be used in the data processing pipeline.\n",
    "- `min_len` and `max_len`: Determine the minimum and maximum duration (in seconds) of the audio segments after decoding. These constraints ensure that each processed audio segment is of a suitable length for the model.\n",
    "- `sampling_rate`: Defines the sampling rate to which the audio should be resampled. This standardizes the input data's sampling rate, making it consistent for model processing.\n",
    "- `extension_time`: Refers to the time (in seconds) by which the duration of an audio event is extended. This parameter is crucial for ensuring that shorter audio events are sufficiently long for the model to process effectively.\n",
    "- `extracted_interval`: Denotes the fixed duration (in seconds) of the audio segment that is randomly extracted from the extended audio event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6723e8-1b8c-4dcd-b55d-0e934f6f704f",
   "metadata": {},
   "source": [
    "Decoding is performed on the fly, ensuring that the data fed into the model is always in the correct format, even when the source data comes in various encoded forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af655211-4a1a-4015-b40f-a6f6fbf2647c",
   "metadata": {},
   "source": [
    "#### 3.4 Feature Extraction\n",
    "\n",
    "Feature extraction is a pivotal step in transforming raw data into a structured format that is suitable for model training. The `DefaultFeatureExtractor` in GADME is tailored for processing waveform data, providing a range of functionalities to prepare the data for model consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbf56c70-dfc1-4bd4-8b91-a0e40ac06380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datamodule.components import DefaultFeatureExtractor\n",
    "feature_extractor = DefaultFeatureExtractor(\n",
    "    feature_size=1,\n",
    "    sampling_rate=32000,\n",
    "    padding_value=0.0,\n",
    "    return_attention_mask=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f84eaf7-0d06-4a5d-91d1-55ede47349f5",
   "metadata": {},
   "source": [
    "Key Parameters:\n",
    "- `feature_size`: Determines the size of the extracted features.\n",
    "- `sampling_rate`: The sampling rate at which the audio data should be processed.\n",
    "- `padding_value`: The value used for padding shorter sequences to a consistent length.\n",
    "- `return_attention_mask`: Indicates whether an attention mask should be returned along with the processed features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae4db7-fa6a-4768-a00a-1e588d0534ac",
   "metadata": {},
   "source": [
    "This component is crucial for ensuring that the input data to the model is in a consistent and processable format, catering to models that require structured input in the form of PyTorch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98891b2a",
   "metadata": {},
   "source": [
    "#### 3.5 No-call Sampler\n",
    "You can use the `NoCallMixer` to add no-call samples to the dataset. This is particularly useful for training models to recognize the absence of bird calls. The `NoCallMixer` is configured as follows:\n",
    "```python\n",
    "from src.datamodule.components.no_call_sampler import NoCallSampler\n",
    "\n",
    "nocall_sampler = NoCallMixer(\n",
    "    directory: str = \"path/to/no_call_samples\",\n",
    "    p: float = 0.075,\n",
    "    sampling_rate: int = 32000,\n",
    "    length: int = 5,\n",
    "    n_classes: int = 21,\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2300978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this would require to have the dataset downloaded (see `download_background_noise.ipynb`, we will not use this for now\n",
    "nocall_sampler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75a91ff-07b7-48f2-a115-7a15de770c52",
   "metadata": {},
   "source": [
    "#### 3.6 Configuration of Data Preprocessing\n",
    "\n",
    "Data preprocessing is a fundamental step in the GADME data pipeline, ensuring that the raw data is adequately conditioned and transformed, making it suitable for model consumption. The `PreprocessingConfig` class allows for a detailed specification of various preprocessing parameters, each carefully selected to meet the unique demands of your dataset and model. Here's how you can configure the data preprocessing in the GADME pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "235ed383-b28f-4575-9a8b-02fff555e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import Spectrogram\n",
    "from src.datamodule.components.resize import Resizer\n",
    "from src.datamodule.components.augmentations import PowerToDB\n",
    "from src.datamodule.components.transforms import PreprocessingConfig\n",
    "\n",
    "# Creating the preprocessing configuration\n",
    "preprocessing = PreprocessingConfig(\n",
    "        spectrogram_conversion= Spectrogram(\n",
    "            n_fft=1024,\n",
    "            hop_length=320,\n",
    "            power=2.0,\n",
    "        ),\n",
    "        resizer=Resizer(\n",
    "            db_scale=True,\n",
    "            target_height=None,\n",
    "            target_width=1024,\n",
    "        ),\n",
    "        dbscale_conversion=PowerToDB(),\n",
    "        normalize_spectrogram=True,\n",
    "        normalize_waveform=None,\n",
    "        mean=4.268, # calculated on AudioSet\n",
    "        std=4.569 # calculated on AudioSet\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eff94aa-3191-41e8-a2ec-4393e8418c94",
   "metadata": {},
   "source": [
    "Here's a brief overview of the parameters used in the `PreprocessingConfig` class:\n",
    "\n",
    "- `spectrogram_conversion`: This is an instance of the Spectrogram class from torchaudio.transforms. It is used to convert the audio waveform to a spectrogram. The parameters n_fft, hop_length, and power are used to configure the spectrogram conversion.\n",
    "\n",
    "    - `n_fft`: Th` size of the FFT, which will also determine the size of the window used for the STFT. It is set to 1024.\n",
    "    - `hop_length`: The number of samples between successive frames in the STFT. It is set to 320.\n",
    "    - `power`: The exponent for the magnitude spectrogram, e.g., 1 for energy, 2 for power, etc. It is set to 2.0.\n",
    "    - `resizer`: This is an instance of the Resizer class from src.datamodule.components.resize. It is used to resize the spectrogram. The parameters db_scale and target_width are used to configure the resizing.\n",
    "\n",
    "- `db_scale`: If set to True, the spectrogram is converted to dB scale. It is set to True.\n",
    "- `target_height`: The target height for the resized spectrogram. It is not set in this case.\n",
    "- target_width: The target width for the resized spectrogram. It is set to 1024.\n",
    "- `dbscale_conversion`: This is an instance of the PowerToDB class from src.datamodule.components.augmentations. It is used to convert the spectrogram to a dB scale.\n",
    "\n",
    "- `normalize_spectrogram`: If set to True, the spectrogram is normalized. It is set to True.\n",
    "\n",
    "- `normalize_waveform`: If set to a value, the audio waveform is normalized. It is not set in this case.\n",
    "\n",
    "- ``mean``: The mean value used for normalization. It is set to 4.268, which is calculated on AudioSet.\n",
    "\n",
    "- `std`: The standard deviation used for normalization. It is set to 4.569, which is calculated on AudioSet.\n",
    "\n",
    "\n",
    "By accurately configuring these preprocessing parameters, you ensure that the input data to the model is standardized and optimized for the learning process, which is essential for achieving high performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d993142c-54a2-43a0-9e4e-b977571b03fd",
   "metadata": {},
   "source": [
    "#### 3.7 Initiating the GADMETransformsWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "267876cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datamodule.components.transforms import GADMETransformsWrapper\n",
    "transforms = GADMETransformsWrapper(\n",
    "    task=\"multilabel\",\n",
    "    sampling_rate=32000,\n",
    "    model_type=\"vision\",\n",
    "    spectrogram_augmentations=spectrogram_augmentations,\n",
    "    waveform_augmentations=waveform_augmentation,\n",
    "    decoding=decoding,\n",
    "    feature_extractor=feature_extractor,\n",
    "    max_length=5,\n",
    "    nocall_sampler=nocall_sampler,\n",
    "    preprocessing=preprocessing,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb170e58-deed-4b52-ba18-160681b5dba0",
   "metadata": {},
   "source": [
    "### 4. Configuration of Event Mappings\n",
    "\n",
    "Event mapping plays a pivotal role in the data pipeline, serving as the bridge between raw dataset events and the structured input required by the model. This process ensures that each event in the dataset is accurately represented and can be effectively utilized during model training. By default, we use [bambird](https://www.sciencedirect.com/science/article/pii/S1574954122004022?casa_token=HEbcdB5MyRMAAAAA:saYbr1WNlJTs-kAZOtzMrNt5r1sN_69E7bMjfCJu2A4zlLLFoIt-5-Cht2Wryg59851H_PWgfHzw) for event mapping, which is implemented in the `XCEventMapping` class. Within the GADME framework, event mappings are configured as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4f30431-6972-4536-b323-88b7378e47c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datamodule.components import XCEventMapping\n",
    "# Instantiate the event mapper\n",
    "mapper = XCEventMapping(\n",
    "            biggest_cluster=True,\n",
    "            no_call=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd405878-31de-4fbc-9729-20410747c89d",
   "metadata": {},
   "source": [
    "Key Parameters in Event Mapping:\n",
    "- `biggest_cluster`: If set to `True`, the mapper focuses on the biggest cluster of events, which can be particularly useful for datasets with imbalanced event distributions.\n",
    "- ``: Specifies the maximum number of events to consider. This can be used to limit the scope of the mapping, although it's usually already managed by the `DatasetConfig`.\n",
    "- `no_call`: Indicates whether 'no-call' events should be included. In this configuration, it's set to `False` as the no-call samples are handled separately by the `nocall_sampler`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedc44f5-852c-4231-bafd-4757ee27841f",
   "metadata": {},
   "source": [
    "Properly configuring the event mappings is essential for ensuring that the model receives accurately structured and meaningful data, which is a cornerstone for effective model training and robust performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704cfdb6-d25c-46b7-a1ee-47d4b834e01f",
   "metadata": {},
   "source": [
    "## Creating the GADME Datamodule\n",
    "\n",
    "The GADME Datamodule plays a central role in the GADME data pipeline, offering streamlined handling and preprocessing of GADME datasets to ensure they are primed for model training. Let's delve into the setup process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60180aa6-285c-4879-abca-e88b29578897",
   "metadata": {},
   "source": [
    "### Imports\n",
    "First, we import the necessary modules. `GADMEDataModule` is responsible for managing the GADME datasets, while the `logging` module is used for logging information during the data processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c76242c-928d-4404-922d-30c462681bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "import os\n",
    "\n",
    "from src.datamodule.gadme_datamodule import GADMEDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29444ab9-1b1c-4ea2-b85c-c55b280f2c16",
   "metadata": {},
   "source": [
    "### Creating Cache Directory\n",
    "The cache directory is a dedicated space for storing processed data. Utilizing a cache directory can significantly expedite subsequent data loading operations by avoiding redundant data processing. Here's how to create and manage a cache directory effectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c8b7b7a-2c44-40d1-a7db-1355d2aefe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the absolute path of the dataset\n",
    "logging.info(f\"Dataset path: <{os.path.abspath(dataset_config.data_dir)}>\")\n",
    "\n",
    "# Create the dataset directory if it does not exist\n",
    "os.makedirs(dataset_config.data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82895c4e-93ac-4590-aeb0-b0acb1f9909b",
   "metadata": {},
   "source": [
    "This approach ensures:\n",
    "- Organized data management: By maintaining a structured directory for your datasets, you facilitate easier access and management of your data assets.\n",
    "- Efficient data loading: By caching processed data, subsequent loads are much faster, which is particularly beneficial when working with large datasets.\n",
    "\n",
    "By carefully setting up the GADME Datamodule and managing your cache directory, you enhance the efficiency and reliability of your data pipeline, ensuring that your datasets are always ready for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d764504c-f167-40b4-9297-6ca1184c5668",
   "metadata": {},
   "source": [
    "### Datamodule Initialization\n",
    "\n",
    "The `GADMEDataModule` class plays a pivotal role in orchestrating the data pipeline. It consolidates the dataset configuration, data loaders, transformations, and event mappings into a cohesive structure, ensuring a clean and manageable workflow. Here's how the GADMEDataModule is initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0586ee6-9cad-4270-acb9-931cf046b6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GADMEDataModule\n",
    "datamodule = GADMEDataModule(\n",
    "        dataset=dataset_config,\n",
    "        loaders=loaders_config,\n",
    "        transforms=transforms,\n",
    "        mapper=mapper,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0982ec-199c-43bc-a2e6-8732f2356cc9",
   "metadata": {},
   "source": [
    "Here's a brief overview of the parameters used in the `GADMEDataModule` class:\n",
    "- `dataset`: The configuration settings for the dataset. It defines how the data is structured and managed.\n",
    "- `loaders`: Configuration settings for the data loaders, determining how data is batched and fed into the model.\n",
    "- `transforms`: The set of transformations and augmentations applied to the data, ensuring that it's properly conditioned for the model.\n",
    "- `mapper`: The event mapping configuration, essential for translating raw dataset events into a structured format that the model can interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab439e-2ee1-471f-9e38-15dfbc12cdfc",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "The data preparation stage is where the actual data processing takes place. This stage is critical in ensuring that the data is correctly preprocessed, structured, and ready for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee400c19-41a3-4a5e-9011-509bdd319e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c1def43c394ac98b6671f2a6424f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files #0:   0%|          | 0/2 [00:00<?, ?obj/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c60c499e8d41129ae339e0bebe7873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files #2:   0%|          | 0/1 [00:00<?, ?obj/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81da5678bbd745b98f9ecfcb08272457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files #1:   0%|          | 0/2 [00:00<?, ?obj/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4503deedf5bf47188d5d11e281ea9e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.21M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05fe528432f04e5ea050a6063f0240e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/96.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b5bef429ff4a54bdab201ae02fa230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbb6ed0bcc447e6a2ccdddd60a59300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/189k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91743d4e052480882d0803c02cf59b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5306889271504f5298e05ae2e60611f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df70003cdcad44a9b0956569c134d1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45be4b930a4a42428ffab4573274a265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dfe93f63c1e4e4cb9e9746ae2ff5fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d345523fa9c4b3e859e6fdff1e10281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/139M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d73181afe24cef9f050da184537095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/213M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b90270ae984dd38c978a39599cf671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/214M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f11f9f987c7401b9974014473713f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/17.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f684538fca429fa86ae23c897021a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files #0:   0%|          | 0/2 [00:00<?, ?obj/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9b462cee9346b48cc58294e7e692ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files #1:   0%|          | 0/2 [00:00<?, ?obj/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55df9cad72e248f791c38782a081a022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files #2:   0%|          | 0/1 [00:00<?, ?obj/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be2a88412184d17b5502e9beafe2fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c9880dc77d4ffb9e89c7a50b58f123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47bd76276c3348619736804677ab91aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_5s split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc2668c55ea4d24b7dddff8d9ba16e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12692ec04e94044882670555f6ac87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38170 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing labels: 100%|██████████| 21/21 [00:02<00:00,  8.23it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0defc0e73744b9fb2eb33ae075d649c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17940 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad1822d6b5649769b47fbc95ff6144e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89721244a8d44a2b5dee01e89655f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14352 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415397fed87546689d3649f4939bef25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3588 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5f643689fb4aca80ddec5e7b812ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare the data for training\n",
    "datamodule.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9cf461-1fb8-4bb4-8db3-f7b4b4830036",
   "metadata": {},
   "source": [
    "The `prepare_data()` method encompasses various steps, including downloading the data (if not already locally available), applying the preprocessing steps defined in the transformations, and organizing the data into a format that is compatible with the model. It's a method that encapsulates the entire data preparation workflow, ensuring that the data is optimally prepared for the training process.\n",
    "\n",
    "This methodical approach to data preparation and modularization of the data pipeline components in the GADME framework contributes significantly to the efficiency, maintainability, and robustness of the machine learning lifecycle.\n",
    "\n",
    "**Hint**: If you recive an error concerning a not existing huggingface dataset, please make sure you are logged in to HuggingFace (see [Log in to Huggingface](#log-in-to-huggingface))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932e3987-0f9a-4452-8b82-54c0ea96180e",
   "metadata": {},
   "source": [
    "### Datamodule Setup for Training Phase\n",
    "\n",
    "Setting up the datamodule for the training phase is a crucial step in the GADME data pipeline. This setup involves initializing the training and validation dataloaders, which play a vital role in supplying the model with data during the training process. The setup is performed using the `setup(stage=\"fit\")` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a252adb3-539e-434c-adcb-2ec2c1f7c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the datamodule for the training phase\n",
    "datamodule.setup(stage=\"fit\")\n",
    "\n",
    "# Retrieve the training and validation dataloaders\n",
    "train_loader = datamodule.train_dataloader()\n",
    "validation_loader = datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2048c240-bac0-465e-836f-1d9718c3a1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_values', 'labels']),\n",
       " torch.Size([32, 1, 128, 1024]),\n",
       " torch.Size([32, 21]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch a sample batch from the training dataloader\n",
    "for batch in train_loader:\n",
    "    break\n",
    "\n",
    "# Inspect the keys and shapes of the data in the batch\n",
    "batch.keys(), batch[\"input_values\"].shape, batch[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00066181-de6f-4980-90dc-458d6b92f27d",
   "metadata": {},
   "source": [
    "This code snippet demonstrates:\n",
    "- The initialization of the training phase.\n",
    "- The retrieval of training and validation dataloaders.\n",
    "- Fetching and inspecting a sample batch from the training dataloader.\n",
    "- The shapes of `input_values` and `labels` indicate the batch size, number of channels (if applicable), and dimensions of the input data and labels, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed731d9c-7aea-49ac-9df9-4fc61774abbc",
   "metadata": {},
   "source": [
    "### Datamodule Setup for Test Phase\n",
    "\n",
    "Similarly, the datamodule is set up for the test phase to ensure that the model can be effectively evaluated using the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5db95322-ca2b-4f8f-b419-40529d7c92b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the datamodule for the test phase\n",
    "datamodule.setup(stage=\"test\")\n",
    "\n",
    "# Retrieve the test dataloader\n",
    "test_loader = datamodule.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5ed904-f95b-4bad-a03b-531de0a67dc7",
   "metadata": {},
   "source": [
    "The `setup(stage=\"test\")` method prepares the datamodule specifically for the test phase, and `test_dataloader()` retrieves the test dataloader, which is instrumental for batching and loading the test data efficiently during the model evaluation process.\n",
    "\n",
    "By methodically setting up the datamodule for both training and test phases, you ensure that the model has access to well-prepared data, which is essential for accurate training, validation, and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2567af44-494a-4547-b57b-9b487e57a520",
   "metadata": {},
   "source": [
    "### Usage in TensorFlow\n",
    "\n",
    "Utilizing the GADME datamodule in a TensorFlow environment involves integrating the prepared dataloaders with TensorFlow's training and evaluation workflows. This integration ensures that the data is fed into TensorFlow models efficiently and in a format that TensorFlow can process. Here's how you can set up the GADME datamodule for TensorFlow compatibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce1c99c0-201a-4cae-9e9e-d4c6f9512fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the datamodule for the training phase\n",
    "datamodule.setup(stage=\"fit\")\n",
    "\n",
    "# Retrieve the training and validation datasets\n",
    "train_loader = datamodule.train_dataset\n",
    "validation_loader = datamodule.val_dataset\n",
    "\n",
    "# Setup the datamodule for the test phase\n",
    "datamodule.setup(stage=\"test\")\n",
    "\n",
    "# Retrieve the test dataset\n",
    "test_loader = datamodule.test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bda355-34f2-42b2-b57a-b6112f48f49c",
   "metadata": {},
   "source": [
    "#### Key Considerations:\n",
    "- `train_dataset`, `validation_dataset`, and `test_dataset` are the datasets prepared by the GADME datamodule, ready to be used in TensorFlow's training and evaluation routines.\n",
    "- It's important to ensure that these datasets are in a format compatible with TensorFlow. This might involve additional steps such as converting the data to `tf.data.Dataset` objects or applying necessary transformations to align with TensorFlow's data handling mechanisms.\n",
    "- More information on this integration process can be found in [HuggingFace's documentation](https://huggingface.co/docs/datasets/use_with_tensorflow).\n",
    "\n",
    "By following these steps, you can leverage the robust data preprocessing and management capabilities of the GADME datamodule within a TensorFlow environment, facilitating an efficient and streamlined model training and evaluation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456bf946-c60a-451b-b290-5249ac68e36b",
   "metadata": {},
   "source": [
    "### Mapping of Labels to eBird Codes\n",
    "\n",
    "The eBird codes in the GADME datasets are in integer format. However, we can map these numeric labels to their corresponding eBird codes as defined in the `dataset_info.json` file (it is created during data preprocessing in the folder where the preprocessed data is stored; i.e. in `data_dir` of the `DatasetConfig`). The `get_label_to_category_mapping_from_metadata` function does this by parsing the JSON file and creating a dictionary that maps each numeric label to its corresponding eBird code in string format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ebe88e2-a74c-4dd5-94ab-9193418f764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def get_label_to_category_mapping_from_metadata(\n",
    "    file_path: str, task: str\n",
    ") -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    Reads a JSON file and extracts the mapping of labels to eBird codes.\n",
    "\n",
    "    The function expects the JSON structure to be in a specific format, where the mapping\n",
    "    is a list of names located under the keys 'features' -> 'labels' -> 'names'.\n",
    "    The index in the list corresponds to the label, and the value at that index is the eBird code.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path to the JSON file containing the label to eBird code mapping.\n",
    "    - task (str): The type of task for which to get the mapping. Expected values are \"multiclass\" or \"multilabel\".\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, str]: A dictionary where each key is a label (integer) and the corresponding value is the eBird code.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If the file at `file_path` does not exist.\n",
    "    - json.JSONDecodeError: If the file is not a valid JSON.\n",
    "    - KeyError: If the expected keys ('features', 'labels', 'names') are not found in the JSON structure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the file and read the JSON data\n",
    "    with open(file_path, \"r\") as file:\n",
    "        dataset_info = json.load(file)\n",
    "\n",
    "    # Extract the list of eBird codes from the loaded JSON structure.\n",
    "    # Note: This assumes a specific structure of the JSON data.\n",
    "    # If the structure is different, this line will raise a KeyError.\n",
    "    if task == \"multiclass\":\n",
    "        ebird_codes_list = dataset_info[\"features\"][\"labels\"][\"names\"]\n",
    "    elif task == \"multilabel\":\n",
    "        ebird_codes_list = dataset_info[\"features\"][\"labels\"][\"feature\"][\"names\"]\n",
    "    else:\n",
    "        # If the task is not recognized (not multiclass or multilabel), raise an error.\n",
    "        raise NotImplementedError(\n",
    "            f\"Only the multiclass and multilabel tasks are implemented, not task {task}.\"\n",
    "        )\n",
    "\n",
    "    # Create a dictionary mapping each label (index) to the corresponding eBird code.\n",
    "    mapping = {label: ebird_code for label, ebird_code in enumerate(ebird_codes_list)}\n",
    "\n",
    "    return mapping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-xS3fZVNL-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
